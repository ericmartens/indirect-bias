{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook demonstrates the concept of automated bias mitigation using IBM Watson OpenScale's auto-debias endpoint.\n\nWe make use of an HR hiring dataset, where the model will be unfairly biased against females.\n\nThe model will be configured as a pre-production model in OpenScale."}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook should be run in a [Watson Studio project](https://dataplatform.cloud.ibm.com/projects/), using a Python 3.7 or above runtime environment. If you are viewing this in Watson Studio and do not see Python 3.7 or above in the upper right corner of your screen, please update the runtime now. It requires the following Cloud services:\n\n - [IBM Watson OpenScale](https://cloud.ibm.com/catalog/services/watson-openscale)\n - [Watson Machine Learning](https://cloud.ibm.com/catalog/services/machine-learning)\n\nIf you have a paid Cloud account, you may also provision a [Databases for PostgreSQL](https://cloud.ibm.com/catalog/services/databases-for-postgresql) or [Db2 Warehouse](https://cloud.ibm.com/catalog/services/db2-warehouse) service to take full advantage of integration with Watson Studio and continuous learning services. If you choose not to provision this paid service, you can use the free internal PostgreSQL storage with OpenScale, but will not be able to configure continuous learning for your model."}, {"metadata": {}, "cell_type": "markdown", "source": "## Credentials for IBM Cloud Services\n\nYour Cloud API key can be generated by going to the [Users section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it between the single quotes in the cell below."}, {"metadata": {}, "cell_type": "code", "source": "CLOUD_API_KEY = \"____CLOUD_API_KEY_HERE____\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create a deployment space\n\n**It is HIGHLY recommended that you create a new space for this project and model, because this script will remove existing spaces as machine learning service providers for OpenScale, which may interfere with other models you are monitoring if they are in the same space.**\n\nAll deployed models require a deployment space. Go to the [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas) to create a new space, or choose an existing one.\n\nClick on the name of the space, then go to the **Settings tab**. Locate the **Space ID** and then click the icon to copy the ID to your clipboard. Paste your space ID between the quotation marks below."}, {"metadata": {}, "cell_type": "code", "source": "WML_SPACE_ID = '____SPACE_ID_HERE____'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Database Credentials\n\nThis tutorial can use Databases for PostgreSQL, Db2 Warehouse, or a free internal version of PostgreSQL to create a datamart for OpenScale. The free internal version can be accessed via the OpenScale APIs, but you will be unable to access it using direct database queries.\n\nIf you have previously configured OpenScale, it will use your existing datamart, and not interfere with any models you are currently monitoring. Do not update the cell below.\n\nIf you do not have a paid Cloud account or would prefer not to provision this paid service, you may use the free internal PostgreSQL service with OpenScale. Do not update the cell below.\n\nTo provision a new instance of Db2 Warehouse, locate [Db2 Warehouse](https://cloud.ibm.com/catalog/services/db2-warehouse) in the Cloud catalog, give your service a name, and click **Create**. Once your instance is created, click the **Service Credentials** link on the left side of the screen. Click the **New credential** button, give your credentials a name, and click **Add**. Your new credentials can be accessed by clicking the **View credentials** button. Copy and paste your Db2 Warehouse credentials into the cell below.\n\nTo provision a new instance of Databases for PostgreSQL, locate [Databases for PostgreSQL](https://cloud.ibm.com/catalog/services/databases-for-postgresql) in the Cloud catalog, give your service a name, and click **Create**. Once your instance is created, click the **Service Credentials** link on the left side of the screen. Click the **New credential** button, give your credentials a name, and click **Add**. Your new credentials can be accessed by clicking the **View credentials** button. Copy and paste your Databases for PostgreSQL credentials into the cell below."}, {"metadata": {}, "cell_type": "code", "source": "DB_CREDENTIALS = None", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Run the notebook\n\nAt this point, you should be able to run the remainder of the notebook without modifications until you get to the section titled, **STOP HERE! Upload and evaluate test data**. The notebook will download training data from github and train a model. The model will be saved to your Watson Machine Learning deployment space and deployed. Finally, OpenScale will be configured to monitor the model."}, {"metadata": {}, "cell_type": "markdown", "source": "## Install packages and import libraries"}, {"metadata": {}, "cell_type": "code", "source": "!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1\n!pip install pyspark==2.4.0 --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nfrom pyspark import SparkContext, SQLContext\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.feature import StringIndexer, VectorAssembler, IndexToString\nfrom pyspark.sql.types import StructType, DoubleType, StringType, ArrayType", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "WML_CREDENTIALS = {\n    \"apikey\": CLOUD_API_KEY,\n    \"url\": 'https://us-south.ml.cloud.ibm.com'\n}\nWML_CREDENTIALS", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Load and explore data"}, {"metadata": {}, "cell_type": "code", "source": "!rm hr_hiring_data.csv\n!wget https://raw.githubusercontent.com/ericmartens/indirect-bias/main/data/hr_training_data.csv", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nfrom pyspark import SparkFiles\n\npd_data = pd.read_csv(\"hr_training_data.csv\", sep=\",\", header=0)\n\nspark = SparkSession.builder.getOrCreate()\nspark_df = spark.createDataFrame(pd_data)\nspark_df.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create the model pipeline and train the model"}, {"metadata": {}, "cell_type": "code", "source": "MODEL_NAME = 'Hiring - Auto Debias'\nDEPLOYMENT_NAME = 'Hiring Deployment - Auto Debias'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Index the categorical fields from the training data"}, {"metadata": {}, "cell_type": "code", "source": "si_Gender = StringIndexer(inputCol='Gender', outputCol='Gender_IX')\nsi_BusinessTravel = StringIndexer(inputCol='BusinessTravel', outputCol='BusinessTravel_IX')\nsi_Department = StringIndexer(inputCol='Department', outputCol='Department_IX')\nsi_Education = StringIndexer(inputCol='Education', outputCol='Education_IX')\nsi_EducationField = StringIndexer(inputCol='EducationField', outputCol='EducationField_IX')\nsi_RelevantEducationLevel = StringIndexer(inputCol='RelevantEducationLevel', outputCol='RelevantEducationLevel_IX')\nsi_JobRole = StringIndexer(inputCol='JobRole', outputCol='JobRole_IX')\nsi_JobLevel = StringIndexer(inputCol='JobLevel', outputCol='JobLevel_IX')\nsi_MaritalStatus = StringIndexer(inputCol='MaritalStatus', outputCol='MaritalStatus_IX')\nsi_OverTime = StringIndexer(inputCol='OverTime', outputCol='OverTime_IX')\nsi_RequestedBenefits = StringIndexer(inputCol='RequestedBenefits', outputCol='RequestedBenefits_IX')\nsi_PreferredSkills = StringIndexer(inputCol='PreferredSkills', outputCol='PreferredSkills_IX')\nsi_JobType = StringIndexer(inputCol='JobType', outputCol='JobType_IX')\nsi_SalaryExpectation = StringIndexer(inputCol='SalaryExpectation', outputCol='SalaryExpectation_IX')\n\nsi_InterviewScore = StringIndexer(inputCol='InterviewScore', outputCol='InterviewScore_IX')\nsi_ResumeScore = StringIndexer(inputCol='ResumeScore', outputCol='ResumeScore_IX')\n\nsi_Label = StringIndexer(inputCol=\"HIRED\", outputCol=\"label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Drop the gender and ethnicity columns, since we will not be using these to train the model. OpenScale will be configured to monitor these values."}, {"metadata": {}, "cell_type": "code", "source": "columns_to_drop = ['Ethnicity']\nspark_df_tmp = spark_df.drop(*columns_to_drop)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Perform the train/test split."}, {"metadata": {}, "cell_type": "code", "source": "(train_data, test_data) = spark_df_tmp.randomSplit([0.9, 0.1], 24)\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "va_features = VectorAssembler(\ninputCols=[\"Age\", \"Gender_IX\", \"BusinessTravel_IX\", \"Department_IX\", \"DistanceFromHome\",\n           \"Education_IX\", \"EducationField_IX\", \"RelevantEducationLevel_IX\", \"JobLevel_IX\", \"JobRole_IX\"\n           , \"MaritalStatus_IX\",\"NumCompaniesWorked\", \"OverTime_IX\",\n           \"InterviewScore_IX\", \"ResumeScore_IX\", \"RequestedBenefits_IX\", \"TotalWorkingYears\", \"PreferredSkills_IX\",\n           \"YearsAtCurrentCompany\",\"RelevantExperience\",\"JobType_IX\",\"SalaryExpectation_IX\"], outputCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "classifier = GBTClassifier(featuresCol=\"features\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Construct the model pipeline"}, {"metadata": {}, "cell_type": "code", "source": "pipeline = Pipeline(stages=[si_Gender, si_BusinessTravel, si_Department,si_Education, si_EducationField,si_RelevantEducationLevel, si_JobRole,si_JobLevel, si_MaritalStatus,\n        si_OverTime,si_InterviewScore,si_ResumeScore,si_RequestedBenefits,si_PreferredSkills,si_JobType,si_SalaryExpectation, si_Label, va_features, classifier, label_converter])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Evaluate the model"}, {"metadata": {}, "cell_type": "code", "source": "model = pipeline.fit(train_data)\npredictions = model.transform(test_data)\nevaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\naccuracy = evaluator.evaluate(predictions)\n\nprint(\"Accuracy = %g\" % accuracy)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Save and deploy the model\n\nBefore we save and deploy the model, we will remove any existing OpenScale subscriptions for this model, as well as any existing deployments, so that the notebook can be run repeatedly to refresh the demo."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_cloud_sdk_core.authenticators import IAMAuthenticator\nfrom ibm_watson_openscale import APIClient\n\nservice_credentials = {\n    \"apikey\": CLOUD_API_KEY,\n    \"url\": \"https://api.aiopenscale.cloud.ibm.com\"\n}\n\nauthenticator = IAMAuthenticator(apikey=service_credentials['apikey'])\n\nwos_client = APIClient(authenticator=authenticator)\nwos_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscriptions = wos_client.subscriptions.list().result.subscriptions\nfor subscription in subscriptions:\n    if subscription.entity.asset.name == MODEL_NAME:\n        print(\"Deleting existing subscription for model\", subscription.entity.asset.name)\n        wos_client.subscriptions.delete(subscription.metadata.id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We can now save and deploy the model."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\nwml_client = APIClient(WML_CREDENTIALS)\nwml_client.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.set.default_space(WML_SPACE_ID)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "training_data_references = [\n                {\n                    \"id\": \"product line\",\n                    \"type\": \"s3\",\n                    \"connection\": {\n                        \"access_key_id\": \"yqcPbWZ0AQPHleHVerrR4Wx5e9pymBdMgydbEra5zCif\",\n                        \"endpoint_url\": \"https://s3.us.cloud-object-storage.appdomain.cloud\",\n                        \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/7d8b3c34272c0980d973d3e40be9e9d2:2883ef10-23f1-4592-8582-2f2ef4973639::\"\n                    },\n                    \"location\": {\n                        \"bucket\": \"faststartlab-donotdelete-pr-nhfd4jnhlxgpc7\",\n                        \"path\": \"hr_training_data.csv\",\n                    }\n                }\n            ]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sw_spec_uid = wml_client.software_specifications.get_uid_by_name(\"spark-mllib_2.4\")\nsw_spec_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Delete any existing versions of this model and deployment"}, {"metadata": {}, "cell_type": "code", "source": "for deployment in wml_client.deployments.get_details()['resources']:\n    deployment_id=deployment['metadata']['id']\n    deployment = wml_client.deployments.get_details(deployment_id)\n    model_uid = deployment['entity']['asset']['id']\n    if deployment['entity']['name'] == DEPLOYMENT_NAME:\n        print('Deleting existing deployment with id', deployment_id)\n        wml_client.deployments.delete(deployment_id)\n        print('Deleting existing model with id', model_uid)\n        wml_client.repository.delete(model_uid) ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Store the model"}, {"metadata": {}, "cell_type": "code", "source": "print(\"Storing model...\")\npublished_model_details = wml_client.repository.store_model(\n    model=model, \n    meta_props={\n        wml_client._models.ConfigurationMetaNames.NAME: \"{}\".format(MODEL_NAME),\n        wml_client._models.ConfigurationMetaNames.SPACE_UID: WML_SPACE_ID,\n        wml_client._models.ConfigurationMetaNames.TYPE: \"mllib_2.4\",\n        wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: sw_spec_uid,\n        wml_client._models.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n      wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"HIRED\",\n    }, \n    training_data=train_data, \n    pipeline=pipeline)\nmodel_uid = wml_client.repository.get_model_uid(published_model_details)\nprint(\"Done\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Deploy the model"}, {"metadata": {}, "cell_type": "code", "source": "deployment_details = wml_client.deployments.create(\n    model_uid, \n    meta_props={\n        wml_client.deployments.ConfigurationMetaNames.NAME: \"{}\".format(DEPLOYMENT_NAME),\n        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n    }\n)\nscoring_url = wml_client.deployments.get_scoring_href(deployment_details)\ndeployment_uid=wml_client.deployments.get_uid(deployment_details)\n\nprint(\"Scoring URL: {}\".format(scoring_url))\nprint(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Configure OpenScale \n\nWe will now configure Watson OpenScale to monitor the deployed model. When this step is finished, all data into and out of the model will be logged, and can be made available to our applications via the Python API. Additionally, we will have the ability to generate explanations for individual predictions.\n\nThe code below creates the OpenScale datamart, a database in which OpenScale will store its data. If you have already set up OpenScale, it will use your existing datamart and not remove any previous data. If you specified Db2 Warehouse or Databases for PostgreSQL credentials above, it will use those credentials to create a datamart with that paid service. Finally, if you have not previously used OpenScale and did not supply credentials for a paid database service, it will create the datamart in a free, internal database. This internal database still allows access via the OpenScale APIs, but you cannot access it directly via database queries."}, {"metadata": {}, "cell_type": "markdown", "source": "## Create schema and datamart\n\n### Set up datamart\nWatson OpenScale uses a database to store payload logs and calculated metrics. If a datamart already exists, OpenScale will use that. If not, and database credentials were not supplied above, the notebook will use the free, internal lite database. If database credentials were supplied, the datamart will be created on that service."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "wos_client.data_marts.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "data_marts = wos_client.data_marts.list().result.data_marts\nif len(data_marts) == 0:\n    if DB_CREDENTIALS is not None:\n        if SCHEMA_NAME is None: \n            print(\"Please specify the SCHEMA_NAME and rerun the cell\")\n\n        print(\"Setting up external datamart\")\n        added_data_mart_result = wos_client.data_marts.add(\n                background_mode=False,\n                name=\"WOS Data Mart\",\n                description=\"Data Mart created by WOS tutorial notebook\",\n                database_configuration=DatabaseConfigurationRequest(\n                  database_type=DatabaseType.POSTGRESQL,\n                    credentials=PrimaryStorageCredentialsLong(\n                        hostname=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"hosts\"][0][\"hostname\"],\n                        username=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"authentication\"][\"username\"],\n                        password=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"authentication\"][\"password\"],\n                        db=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"database\"],\n                        port=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"hosts\"][0][\"port\"],\n                        ssl=True,\n                        sslmode=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"query_options\"][\"sslmode\"],\n                        certificate_base64=DB_CREDENTIALS[\"connection\"][\"postgres\"][\"certificate\"][\"certificate_base64\"]\n                    ),\n                    location=LocationSchemaName(\n                        schema_name= SCHEMA_NAME\n                    )\n                )\n             ).result\n    else:\n        print(\"Setting up internal datamart\")\n        added_data_mart_result = wos_client.data_marts.add(\n                background_mode=False,\n                name=\"WOS Data Mart\",\n                description=\"Data Mart created by WOS tutorial notebook\", \n                internal_database = True).result\n        \n    data_mart_id = added_data_mart_result.metadata.id\n    \nelse:\n    data_mart_id=data_marts[0].metadata.id\n    print(\"Using existing datamart {}\".format(data_mart_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Bind WML machine learning instance as Pre-Prod\n\nWatson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model. If a binding with name \"Watson Machine Learning OpenScale Demo\" already exists, this code will delete that binding a create a new one."}, {"metadata": {}, "cell_type": "code", "source": "wos_client.service_providers.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "SERVICE_PROVIDER_NAME = \"WMLearning OpenScale Demo\"\nSERVICE_PROVIDER_DESCRIPTION = \"Added by tutorial WOS notebook.\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "service_providers = wos_client.service_providers.list().result.service_providers\nfor service_provider in service_providers:\n    service_instance_name = service_provider.entity.name\n    if service_instance_name == SERVICE_PROVIDER_NAME:\n        service_provider_id = service_provider.metadata.id\n        wos_client.service_providers.delete(service_provider_id)\n        print(\"Deleted existing service_provider for WML instance: {}\".format(service_provider_id))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_openscale.supporting_classes.enums import *\nfrom ibm_watson_openscale.supporting_classes import *\n\nadded_service_provider_result = wos_client.service_providers.add(\n        name=SERVICE_PROVIDER_NAME,\n        description=SERVICE_PROVIDER_DESCRIPTION,\n        service_type=ServiceTypes.WATSON_MACHINE_LEARNING,\n        deployment_space_id = WML_SPACE_ID,\n        operational_space_id = \"pre_production\",\n        credentials=WMLCredentialsCloud(\n            apikey=CLOUD_API_KEY,      \n            url=WML_CREDENTIALS[\"url\"],\n            instance_id=None\n        ),\n        background_mode=False\n    ).result\nservice_provider_id = added_service_provider_result.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "asset_deployment_details = wos_client.service_providers.list_assets(data_mart_id=data_mart_id, service_provider_id=service_provider_id, deployment_id=deployment_uid, deployment_space_id = WML_SPACE_ID).result['resources'][0]\nasset_deployment_details", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Set up the model for monitoring in OpenScale"}, {"metadata": {}, "cell_type": "code", "source": "feature_columns=list(pd_data.drop(['HIRED','Ethnicity'],axis=1))\nfeature_columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "categorical_features = pd_data[feature_columns].select_dtypes(include=['object']).columns.tolist()\ncategorical_features", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_asset_details_from_deployment=wos_client.service_providers.get_deployment_asset(data_mart_id=data_mart_id,service_provider_id=service_provider_id,deployment_id=deployment_uid,deployment_space_id=WML_SPACE_ID)\nmodel_asset_details_from_deployment", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "subscription_details = wos_client.subscriptions.add(\n        data_mart_id=data_mart_id,\n        service_provider_id=service_provider_id,\n        asset=Asset(\n            asset_id=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"asset_id\"],\n            name=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"name\"],\n            url=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"url\"],\n            asset_type=AssetTypes.MODEL,\n            input_data_type=InputDataType.STRUCTURED,\n            problem_type=ProblemType.BINARY_CLASSIFICATION\n        ),\n        deployment=AssetDeploymentRequest(\n            deployment_id=asset_deployment_details[\"metadata\"][\"guid\"],\n            name=asset_deployment_details[\"entity\"][\"name\"],\n            deployment_type= DeploymentTypes.ONLINE,\n            url=asset_deployment_details['entity']['scoring_endpoint']['url']\n        ),\n        asset_properties=AssetPropertiesRequest(\n            label_column=\"HIRED\",\n            probability_fields=[\"probability\"],\n            prediction_field=\"predictedLabel\",\n            feature_fields = feature_columns,\n            categorical_fields = categorical_features,\n            training_data_reference=TrainingDataReference(type=\"cos\",\n                                                          location=COSTrainingDataReferenceLocation(bucket=\"faststartlab-donotdelete-pr-nhfd4jnhlxgpc7\",\n                                                                                                    file_name=\"hr_training_data.csv\"),\n                                                          connection=COSTrainingDataReferenceConnection.from_dict({\n                                                                        \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/7d8b3c34272c0980d973d3e40be9e9d2:2883ef10-23f1-4592-8582-2f2ef4973639::\",\n                                                                        \"url\": \"https://s3.us.cloud-object-storage.appdomain.cloud\",\n                                                                        \"api_key\": \"yqcPbWZ0AQPHleHVerrR4Wx5e9pymBdMgydbEra5zCif\",\n                                                                        \"iam_url\": \"https://iam.bluemix.net/oidc/token\"})),\n            training_data_schema=SparkStruct.from_dict(model_asset_details_from_deployment[\"entity\"][\"asset_properties\"][\"training_data_schema\"])\n        )\n    ).result\nsubscription_id = subscription_details.metadata.id\nprint(subscription_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Score the model so we can configure monitors\n\nNow that the WML service has been bound and the subscription has been created, we need to send a request to the model before we configure OpenScale. This allows OpenScale to create a payload log in the datamart with the correct schema, so it can capture data coming into and out of the model. First, the code gets the model deployment's endpoint URL, and then sends a few record for predictions."}, {"metadata": {}, "cell_type": "markdown", "source": "## Get payload data for scoring the model\n\nDownload the payload data from github, and split out the protected attributes and the prediction column."}, {"metadata": {}, "cell_type": "code", "source": "!rm payload_100.csv\n!wget https://raw.githubusercontent.com/ericmartens/indirect-bias/main/data/payload_100.csv", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "payload_data = pd.read_csv(\"payload_100.csv\", sep=\",\", header=0)\nprotected_attributes=['Ethnicity']\n\ncols_to_remove = ['HIRED']\ncols_to_remove.extend(protected_attributes)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Construct the scoring payload\n\nThe scoring payload includes the protected data (gender and ethnicity) in the metadata, so it can be logged by OpenScale, but not passed to the model for scoring."}, {"metadata": {}, "cell_type": "code", "source": "def get_scoring_payload(no_of_records_to_score = 1):\n\n    for col in cols_to_remove:\n        if col in payload_data.columns:\n            del payload_data[col] \n\n    fields = payload_data.columns.tolist()\n    values = payload_data[fields].values.tolist()\n\n    payload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values[:no_of_records_to_score],}]}\n \n    return payload_scoring", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Method to perform scoring"}, {"metadata": {}, "cell_type": "code", "source": "def sample_scoring(no_of_records_to_score = 1):\n    records_list=[]\n    payload_scoring = get_scoring_payload(no_of_records_to_score)\n    \n    scoring_response = wml_client.deployments.score(deployment_uid, payload_scoring)\n    print('Single record scoring result:', '\\n fields:', scoring_response['predictions'][0]['fields'], '\\n values: ', scoring_response['predictions'][0]['values'][0])\n    print(json.dumps(scoring_response, indent=None))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Score the model and print the scoring response"}, {"metadata": {}, "cell_type": "code", "source": "import json\nsample_scoring(no_of_records_to_score=100)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Validate that the payload scoring worked by checking the payload data set ID"}, {"metadata": {}, "cell_type": "code", "source": "import time\n\ntime.sleep(10)\npayload_data_set_id = None\npayload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n                                                target_target_id=subscription_id, \n                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\nif payload_data_set_id is None:\n    print(\"Payload data set not found. Please check subscription status.\")\nelse:\n    print(\"Payload data set id:\", payload_data_set_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Fairness, drift monitoring and explanations\n\n###  Fairness configuration\n\nThe code below configures fairness monitoring for our model. It turns on monitoring for two features, sex and age. In each case, we must specify:\n\n - Which model feature to monitor One or more majority groups\n - Which are values of that feature that we expect to receive a higher percentage of favorable outcomes\n - One or more minority groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes\n - The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 80%)\n \nAdditionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 100 records have been added."}, {"metadata": {}, "cell_type": "markdown", "source": "### Create Fairness Monitor Instance"}, {"metadata": {}, "cell_type": "code", "source": "target = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n)\nparameters = {\n    \"features\": [\n        {\n                \"feature\": \"Gender\",\n                \"majority\": [\"Male\"],\n                \"minority\": [\"Female\"]\n        }\n    ],\n    \"favourable_class\": [\"YES\"],\n    \"unfavourable_class\": [\"NO\"],\n    \"min_records\": 100\n}\nthresholds = [\n    {\n        \"metric_id\": \"fairness_value\",\n        \"specific_values\": [\n           {\n                \"applies_to\": [\n                    {\n                        \"type\": \"tag\",\n                        \"value\": \"Gender\",\n                        \"key\": \"feature\"\n                    }\n                ],\n                \"value\": 98\n            }\n        ],\n        \"type\": \"lower_limit\",\n        \"value\": 98\n    }\n]\nfairness_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.FAIRNESS.ID,\n    target=target,\n    parameters=parameters,\n    thresholds=thresholds\n).result\n\nfairness_monitor_instance_id = fairness_monitor_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Drift configuration"}, {"metadata": {}, "cell_type": "code", "source": "monitor_instances = wos_client.monitor_instances.list().result.monitor_instances\nfor monitor_instance in monitor_instances:\n    monitor_def_id=monitor_instance.entity.monitor_definition_id\n    if monitor_def_id == \"drift\" and monitor_instance.entity.target.target_id == subscription_id:\n        wos_client.monitor_instances.delete(monitor_instance.metadata.id)\n        print('Deleted existing drift monitor instance with id: ', monitor_instance.metadata.id)\n\n\ntarget = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n\n)\n\nparameters = {\n    \"min_samples\": 100,\n    \"drift_threshold\": 0.05,\n    \"train_drift_model\": True,\n    \"enable_model_drift\": True,\n    \"enable_data_drift\": True\n}\n\ndrift_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.DRIFT.ID,\n    target=target,\n    parameters=parameters\n).result\n\ndrift_monitor_instance_id = drift_monitor_details.metadata.id\ndrift_monitor_instance_id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Enable quality monitoring\nThe code below turns on the quality (accuracy) monitor and sets an alert threshold of 80%. OpenScale will show an alert on the dashboard if the model accuracy measurement (area under the curve, in the case of a binary classifier) falls below this threshold.\n\nThe second paramater supplied, min_records, specifies the minimum number of feedback records OpenScale needs before it calculates a new measurement. The quality monitor runs hourly, but the accuracy reading in the dashboard will not change until an additional 50 feedback records have been added, via the user interface, the Python client, or the supplied feedback endpoint."}, {"metadata": {}, "cell_type": "code", "source": "target = Target(\n        target_type=TargetTypes.SUBSCRIPTION,\n        target_id=subscription_id\n)\nparameters = {\n    \"min_feedback_data_size\": 50\n}\nthresholds = [\n    {\n        \"metric_id\": \"area_under_roc\",\n        \"type\": \"lower_limit\",\n        \"value\": 0.8\n    }\n]\nquality_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.QUALITY.ID,\n    target=target,\n    parameters=parameters,\n    thresholds=thresholds \n).result\nquality_monitor_instance_id = quality_monitor_details.metadata.id\nquality_monitor_instance_id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Configure Explainability\nFinally, we provide OpenScale with the training data to enable and configure the explainability features."}, {"metadata": {}, "cell_type": "code", "source": "target = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n)\nparameters = {\n    \"enabled\": True\n}\nexplainability_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.EXPLAINABILITY.ID,\n    target=target,\n    parameters=parameters\n).result\n\nexplainability_monitor_id = explainability_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# STOP HERE! Upload and evaluate test data\n\nAt this point, you can navigate to the [OpenScale Insights Dashboard](https://aiopenscale.cloud.ibm.com/aiopenscale/) and select the **Hiring Deployment - Challenger** model. From the **Actions** menu, choose **Evaluate now**. Select **from CSV** from the **Import** dropdown, and upload the [auto_debias_payload_100.csv](https://raw.githubusercontent.com/ericmartens/indirect-bias/main/data/auto_debias_payload_100.csv) file. Then click **Upload and evaluate**. The monitors will take a few minutes to run, but when the screen refreshes, you will see information on the test results for fairness, quality and drift, along with two generated explanations.\n\nWhen the tests have finished running, you may continue with the steps below to test the auto-debiased endpoint."}, {"metadata": {}, "cell_type": "markdown", "source": "## Test the model and auto-debiased endpoing results using a record that will produce a biased result\n\nNow that we've run the model test, OpenScale's auto-debiased endpoint is active. We can see how this works by sending records to the deployed model that will produce a biased result, then sending the same records to the debiased endpoint and seeing that we get a different prediction.\n\nFirst, we'll get the scoring endpoint for the deployed model."}, {"metadata": {}, "cell_type": "code", "source": "subscriptions = wos_client.subscriptions.list().result.subscriptions\n\nspace_id = None\ndeployment_id = None\ndatamart_id = None\nsubscription_id = None\nfor subscription in subscriptions:\n    if subscription.entity.deployment.name == DEPLOYMENT_NAME:\n        deployment_id = subscription.entity.deployment.deployment_id\n        space_id = subscription.entity.asset.url.split('?space_id=')[1].split('&version')[0]\n        datamart_id = subscription.entity.data_mart_id\n        subscription_id = subscription.metadata.id\n        print(\"Deployment ID:\", deployment_id)\n        print(\"Space ID:\", space_id)\n        print(\"Datamart ID:\", datamart_id)\n        print(\"Subscription ID:\", subscription_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "fields = ['Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'Education',\n          'EducationField', 'RelevantEducationLevel', 'JobLevel', 'JobRole', 'MaritalStatus', 'NumCompaniesWorked', 'OverTime', 'InterviewScore',\n          'ResumeScore', 'RequestedBenefits', 'TotalWorkingYears', 'PreferredSkills', 'YearsAtCurrentCompany', 'RelevantExperience', 'JobType', 'SalaryExpectation', 'Gender']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Next, we'll get an authentication token so we can use the auto-debiased endpoint."}, {"metadata": {}, "cell_type": "code", "source": "import json\nimport requests\nimport base64\nfrom requests.auth import HTTPBasicAuth\nimport time\n\nheaders = {}\nheaders[\"Content-Type\"] = \"application/x-www-form-urlencoded\"\nheaders[\"Accept\"] = \"application/json\"\nauth = HTTPBasicAuth(\"bx\", \"bx\")\ndata = {\n    \"grant_type\": \"urn:ibm:params:oauth:grant-type:apikey\",\n    \"apikey\": CLOUD_API_KEY\n}\nresponse = requests.post(\"https://iam.ng.bluemix.net/oidc/token\", data=data, headers=headers, auth=auth)\njson_data = response.json()\niam_access_token = json_data['access_token']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "iam_access_token", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Next, we'll get fifteen records to send to our production model and our debiased endpoint."}, {"metadata": {}, "cell_type": "code", "source": "no_of_records_to_score = 15\nscoring_payload = get_scoring_payload(no_of_records_to_score)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Send the fiften records to the production model for scoring."}, {"metadata": {}, "cell_type": "code", "source": "scoring_response = wml_client.deployments.score(deployment_id, scoring_payload)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in range(no_of_records_to_score):\n    print(scoring_response['predictions'][0]['values'][i][-1:][0])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "URL = \"https://api.aiopenscale.cloud.ibm.com/openscale/{0}/v2/subscriptions/{1}/predictions\".format(datamart_id,subscription_id)\n\nheaders = {}\nheaders[\"Content-Type\"] = \"application/json\"\nheaders[\"Accept\"] = \"application/json\"\nheaders[\"Authorization\"] = \"Bearer {}\".format(iam_access_token)\n\ndebiased_scoring_payload = scoring_payload['input_data'][0]\n\nresponse = requests.post(URL, data=json.dumps(debiased_scoring_payload), headers=headers)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "predictedLabel_index = response.json()['fields'].index('predictedLabel')\ndebiased_prediction_index = response.json()['fields'].index('debiased_prediction')\n\nfor j in range(no_of_records_to_score):\n    scored_record = response.json()['values'][j]\n    predictedLabel = scored_record[predictedLabel_index]\n    debiased_prediction = scored_record[debiased_prediction_index]\n    if predictedLabel != debiased_prediction:\n        print('==========')\n        print(scored_record)\n        print('predictedLabel:' + str(predictedLabel) + ', debiased_prediction=' + str(debiased_prediction))\n        print('==========')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}