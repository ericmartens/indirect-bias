{"cells": [{"metadata": {"collapsed": true, "id": "75feab07-55ef-4436-ab6c-48d5c2d133e4"}, "cell_type": "markdown", "source": "<img src=\"https://github.com/pmservice/ai-openscale-tutorials/raw/master/notebooks/images/banner.png\" align=\"left\" alt=\"banner\">"}, {"metadata": {"id": "b18c0eec7d1c46f6b8fe4c779c478b7b"}, "cell_type": "markdown", "source": "# Working with Watson Machine Learning"}, {"metadata": {"id": "fc3ebd630e524b3e812e2d17d603e5c9"}, "cell_type": "markdown", "source": "This notebook demonstrates the concept of indirect bias detection and automated bias mitigation using IBM Watson OpenScale's auto-debias endpoint.\n\nWe make use of an HR hiring dataset, where the model will be unfairly biased against females and minorities. These two features are not used in the training of the model, and do not affect the prediction. However, their values are submitted as metadata with the prediction request, and are logged in the OpenScale datamart.\n\nUsing the training data, OpenScale detects correlations between the protected features and the model training features. OpenScale can then monitor the protected features for unfair bias, and optionally attempt to remove that bias by perturbing the correlated features if the prediction is submitted to the OpenScale-generated debiased endpoint.\n\nThe model will be configured as a pre-production model in OpenScale."}, {"metadata": {"id": "d8a6988ecc54421b814c40316dfcc528"}, "cell_type": "markdown", "source": "This notebook should be run in a Watson Studio project, using a Python 3.7 or above runtime environment. If you are viewing this in Watson Studio and do not see Python 3.7 or above in the upper right corner of your screen, please update the runtime now. It requires the following services:\n\n - IBM Watson OpenScale, configured to use a database on the cluster\n - Watson Machine Learning"}, {"metadata": {"id": "262ffb75c64b4523bfda24ea95d4b8f5"}, "cell_type": "markdown", "source": "# Setup <a name=\"setup\"></a>"}, {"metadata": {"id": "8624f0f406f04a4a8db20b2d65d0fc11"}, "cell_type": "markdown", "source": "## Create a deployment space\n\n**It is HIGHLY recommended that you create a new space for this project and model, because this script will remove existing spaces as machine learning service providers for OpenScale, which may interfere with other models you are monitoring if they are in the same space.**\n\nAll deployed models require a deployment space. Go to the **Deployments** section of your cluster to create a new space, or choose an existing one.\n\nClick on the name of the space, then go to the **Settings tab**. Locate the **Space ID** and then click the icon to copy the ID to your clipboard. Paste your space ID between the quotation marks below."}, {"metadata": {"id": "d97929b6ba4c435d80cd2a35eb125eec"}, "cell_type": "code", "source": "WML_SPACE_ID = '___PASTE_HERE___'", "execution_count": null, "outputs": []}, {"metadata": {"id": "339a3891b1ab46999282ebca3a14d52c"}, "cell_type": "markdown", "source": "## Enter login credentials\n\nPaste your credentials for the cluster below. They will be used to log into services for deploying the model and configuring OpenScale."}, {"metadata": {"id": "98550e2e7c464b7bbb36eb3538963eeb"}, "cell_type": "code", "source": "WOS_CREDENTIALS = {\n    \"url\": \"___CLUSTER_URL___\",\n    \"username\": \"___USERNAME___\",\n    \"password\": \"___PASSWORD___\",\n    \"version\": \"3.5\"\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "3b7c0a42ac1e43139057e5d78abfe075"}, "cell_type": "markdown", "source": "## Package installation"}, {"metadata": {"id": "827d92b4b8e1491e81d23153816f555c"}, "cell_type": "code", "source": "import warnings\nwarnings.filterwarnings('ignore')", "execution_count": null, "outputs": []}, {"metadata": {"id": "dc441429a8ad4aada3fa29d81e98e5cf"}, "cell_type": "code", "source": "!pip install --upgrade pyspark==2.4 --no-cache | tail -n 1\n\n!pip install --upgrade pandas==0.25.3 --no-cache | tail -n 1\n!pip install --upgrade requests==2.23 --no-cache | tail -n 1\n!pip install numpy==1.16.4 --no-cache | tail -n 1\n!pip install SciPy --no-cache | tail -n 1\n!pip install lime --no-cache | tail -n 1\n!pip install ibm-cloud-sdk-core --no-cache | tail -n 1\n\n!pip install --upgrade ibm-watson-machine-learning --user | tail -n 1\n!pip install --upgrade ibm-watson-openscale --no-cache | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {"id": "160fa6404a9b47c3807050926804a06f"}, "cell_type": "markdown", "source": "## Run the notebook\n\nAt this point, you can run the notebook step-by-step until you reach the instructions telling you to upload test data to OpenScale. If you encounter errors, restarting the kernel here and re-running may resolve them."}, {"metadata": {"id": "e898b417dc8c43bcbcb1b29c08cde8ed"}, "cell_type": "code", "source": "import os\nimport base64\nimport json\nimport requests\nfrom requests.auth import HTTPBasicAuth", "execution_count": null, "outputs": []}, {"metadata": {"id": "d764ead05ec847c780faf44e7a2fd2b2"}, "cell_type": "code", "source": "token = os.environ['USER_ACCESS_TOKEN']\n\nWML_CREDENTIALS = {\n   \"token\" : token,\n   \"instance_id\" : \"wml_local\",\n   \"url\": WOS_CREDENTIALS[\"url\"],\n   \"version\": \"3.5\"\n}", "execution_count": null, "outputs": []}, {"metadata": {"id": "d85650ef53984d799a58b8ce79160674"}, "cell_type": "markdown", "source": "## Training data in Cloud Object Storage\n\n### Cloud object storage details\u00b6\n\nThe next cells contain the location of the model training data in cloud object storage. OpenScale needs access to the data for calculations on protected value correlations with model features, drift data, explainability, and more."}, {"metadata": {"id": "9cb6e02e0589449b9d2a9813d44dec07"}, "cell_type": "code", "source": "IAM_URL = \"https://iam.ng.bluemix.net/oidc/token\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "dbac5a57926d4f7eb3e74564cc6a95af"}, "cell_type": "code", "source": "# masked\nCOS_API_KEY_ID = \"yqcPbWZ0AQPHleHVerrR4Wx5e9pymBdMgydbEra5zCif\"\nCOS_RESOURCE_CRN = \"crn:v1:bluemix:public:cloud-object-storage:global:a/7d8b3c34272c0980d973d3e40be9e9d2:2883ef10-23f1-4592-8582-2f2ef4973639::\"\nCOS_ENDPOINT = \"https://s3.us.cloud-object-storage.appdomain.cloud\"\nBUCKET_NAME = \"faststartlab-donotdelete-pr-nhfd4jnhlxgpc7\"\nFILE_NAME = \"hr_training_data.csv\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "056e2d289da0487c83e16983cde997ec"}, "cell_type": "markdown", "source": "# Load and explore data"}, {"metadata": {"id": "a74cc3aca27d406d88a25ab74a373157"}, "cell_type": "code", "source": "!rm hr_training_data.csv\n!wget https://raw.githubusercontent.com/ericmartens/indirect-bias/main/data/hr_training_data.csv", "execution_count": null, "outputs": []}, {"metadata": {"id": "b2235027af5a4d329972a869fbcfd2f8"}, "cell_type": "markdown", "source": "## Explore data"}, {"metadata": {"id": "b6eec247a77146d88e6476fc5ad49b03"}, "cell_type": "code", "source": "from pyspark.sql import SparkSession\nimport json\n\nspark = SparkSession.builder.getOrCreate()\ndf_data = spark.read.csv(path=\"hr_training_data.csv\", sep=\",\", header=True, inferSchema=True) \ndf_data.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "a09d9e346bce47a98a7eccc833112128"}, "cell_type": "code", "source": "print(\"Number of records: \" + str(df_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {"id": "9d55078678a94079b67b183e549fc534"}, "cell_type": "markdown", "source": "# Create a model"}, {"metadata": {"id": "88357d1f249f427491163b19f6acc9ab"}, "cell_type": "code", "source": "spark_df = df_data\nprotected_attributes = [\"Ethnicity\", \"Gender\"]\nfor attr in protected_attributes:\n    spark_df = spark_df.drop(attr)\ncolumns = spark_df.columns\nmodel_name = \"Hiring Helper\"\ndeployment_name = \"Hiring Helper Deployment\"\n\nspark_df.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {"id": "82155d92dee14d8596d23529d454c3e9"}, "cell_type": "code", "source": "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, IndexToString, VectorAssembler\nfrom pyspark.ml import Pipeline, Model\n\ncat_features = ['BusinessTravel', 'Department', 'Education', 'EducationField', 'RelevantEducationLevel', 'JobRole', 'JobLevel',\\\n                'MaritalStatus', 'OverTime', 'RequestedBenefits', 'PreferredSkills', 'JobType', 'SalaryExpectation',\\\n                'InterviewScore', 'ResumeScore'] \nnum_features = ['Age', 'DistanceFromHome', 'NumCompaniesWorked', 'TotalWorkingYears', 'YearsAtCurrentCompany', 'RelevantExperience']\nstages=[]\n\nfor feature in cat_features:\n    string_indexer = StringIndexer(inputCol = feature, outputCol = feature + '_IX').setHandleInvalid(\"keep\")\n    encoder = OneHotEncoderEstimator(inputCols=[string_indexer.getOutputCol()], outputCols=[feature + \"classVec\"])\n    stages += [string_indexer, encoder]\n\nsi_Label = StringIndexer(inputCol=\"HIRED\", outputCol=\"encoded_label\").fit(spark_df)\nlabel_converter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\", labels=si_Label.labels)\nstages.append(si_Label)", "execution_count": null, "outputs": []}, {"metadata": {"id": "cadc3aae47ca464abf9c956375b1ec72"}, "cell_type": "code", "source": "assembler_inputs = [c + \"classVec\" for c in cat_features] + num_features\nva_features = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")\nstages.append(va_features)", "execution_count": null, "outputs": []}, {"metadata": {"id": "a3d7ff9894674a94bd713d2e9af4cd55"}, "cell_type": "code", "source": "(train_data, test_data) = spark_df.randomSplit([0.8, 0.2], 24)\nprint(\"Number of records for training: \" + str(train_data.count()))\nprint(\"Number of records for evaluation: \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {"id": "720b9ee7712b450c8b3ecdcd4ab58893"}, "cell_type": "code", "source": "train_data.columns", "execution_count": null, "outputs": []}, {"metadata": {"id": "04cf8f5bdf08438f8992890b1bc70442"}, "cell_type": "code", "source": "from pyspark.ml.classification import GBTClassifier, DecisionTreeClassifier, RandomForestClassifier\nclassifier = RandomForestClassifier(labelCol=\"encoded_label\", featuresCol=\"features\")\nstages.append(classifier)\nstages.append(label_converter)\npipeline = Pipeline(stages=stages)\nmodel = pipeline.fit(train_data)", "execution_count": null, "outputs": []}, {"metadata": {"id": "30adcf7239c54415a6e74250a6f80903"}, "cell_type": "code", "source": "predictions = model.transform(test_data)\npredictions.printSchema()\npredictions.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "efa502d57f284612b95903f27157992f"}, "cell_type": "code", "source": "from pyspark.ml.evaluation import BinaryClassificationEvaluator\nevaluatorDT = BinaryClassificationEvaluator(labelCol=\"encoded_label\", rawPredictionCol=\"rawPrediction\")\naccuracy = evaluatorDT.evaluate(predictions)\n\nprint(\"Accuracy = %g\" % accuracy)", "execution_count": null, "outputs": []}, {"metadata": {"id": "bbb45c2f139940b28e3f9dfff4e99576"}, "cell_type": "markdown", "source": "# Save and deploy the model"}, {"metadata": {"id": "8c46867b3f484ba1a396f364742552de"}, "cell_type": "code", "source": "import json\nfrom ibm_watson_machine_learning import APIClient\n\nwml_client = APIClient(WML_CREDENTIALS)\nwml_client.version", "execution_count": null, "outputs": []}, {"metadata": {"id": "5d6754453fdb4e50836da3e9ed32df17"}, "cell_type": "code", "source": "wml_client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {"id": "f1cd9533f53a414f86f748e34275cfce"}, "cell_type": "markdown", "source": "## Set the default space specified earlier in the notebook"}, {"metadata": {"id": "3393075295af4022a7f060b5a85a5b6e"}, "cell_type": "code", "source": "wml_client.set.default_space(WML_SPACE_ID)", "execution_count": null, "outputs": []}, {"metadata": {"id": "7f4015490dfc48c7a7b74c7ed9627284"}, "cell_type": "code", "source": "deployments_list = wml_client.deployments.get_details()\nfor deployment in deployments_list[\"resources\"]:\n    model_id = deployment[\"entity\"][\"asset\"][\"id\"]\n    deployment_id = deployment[\"metadata\"][\"id\"]\n    if deployment[\"metadata\"][\"name\"] == deployment_name:\n        print(\"Deleting deployment id\", deployment_id)\n        wml_client.deployments.delete(deployment_id)\n        print(\"Deleting model id\", model_id)\n        wml_client.repository.delete(model_id)\nwml_client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {"id": "e9df60c1cc314ccb887411f0d3253a92"}, "cell_type": "code", "source": "training_data_references = [\n                {\n                    \"id\": \"product line\",\n                    \"type\": \"s3\",\n                    \"connection\": {\n                        \"access_key_id\": COS_API_KEY_ID,\n                        \"endpoint_url\": COS_ENDPOINT,\n                        \"resource_instance_id\":COS_RESOURCE_CRN\n                    },\n                    \"location\": {\n                        \"bucket\": BUCKET_NAME,\n                        \"path\": FILE_NAME,\n                    }\n                }\n            ]", "execution_count": null, "outputs": []}, {"metadata": {"id": "cf0293f9272c4623a4c39f752b6554cf"}, "cell_type": "code", "source": "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_2.4\")\nprint(\"Software Specification ID: {}\".format(software_spec_uid))\nmodel_props = {\n        wml_client._models.ConfigurationMetaNames.NAME:\"{}\".format(model_name),\n        wml_client._models.ConfigurationMetaNames.SPACE_UID: WML_SPACE_ID,\n        wml_client._models.ConfigurationMetaNames.TYPE: \"mllib_2.4\",\n        wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n        wml_client._models.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n        wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"HIRED\",\n    }", "execution_count": null, "outputs": []}, {"metadata": {"id": "e7d3327cef8c43c18ff8191f930f887d"}, "cell_type": "code", "source": "print(\"Storing model ...\")\npublished_model_details = wml_client.repository.store_model(\n    model=model, \n    meta_props=model_props, \n    training_data=train_data, \n    pipeline=pipeline)\n\nmodel_uid = wml_client.repository.get_model_uid(published_model_details)\nprint(\"Done\")\nprint(\"Model ID: {}\".format(model_uid))", "execution_count": null, "outputs": []}, {"metadata": {"id": "06c2bb55cbf04f59b092c8d773e7952b"}, "cell_type": "code", "source": "published_model_details", "execution_count": null, "outputs": []}, {"metadata": {"id": "f5483e2532fe43c1b931b1e18aa246d0"}, "cell_type": "markdown", "source": "## Create a model deployment"}, {"metadata": {"id": "cdc6ce1dcab84ca2803daa6444eaea6f"}, "cell_type": "code", "source": "deployment_details = wml_client.deployments.create(\n    model_uid, \n    meta_props={\n        wml_client.deployments.ConfigurationMetaNames.NAME: \"{}\".format(deployment_name),\n        wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n    }\n)\nscoring_url = wml_client.deployments.get_scoring_href(deployment_details)\ndeployment_uid=wml_client.deployments.get_uid(deployment_details)\n\nprint(\"Scoring URL:\" + scoring_url)\nprint(\"Model id: {}\".format(model_uid))\nprint(\"Deployment id: {}\".format(deployment_uid))", "execution_count": null, "outputs": []}, {"metadata": {"id": "0fa3b287605044538f3c1b3ee632eb9b"}, "cell_type": "markdown", "source": "# Construct the scoring payload"}, {"metadata": {"id": "eb1e48ae98514c14a6dc225eba1acc9f"}, "cell_type": "code", "source": "import pandas as pd\n\ndf = pd.read_csv(\"hr_training_data.csv\")\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {"id": "a8aaab665f9947b594380d4892c237a7"}, "cell_type": "markdown", "source": "## Remove the sensitive attributes"}, {"metadata": {"id": "dcd4da086e604f92b7bedd6f1283ba4e"}, "cell_type": "code", "source": "cols_to_remove = ['HIRED']\ncols_to_remove.extend(protected_attributes)\ncols_to_remove", "execution_count": null, "outputs": []}, {"metadata": {"id": "546d7ce9df5c404f84c2a556c3679e4e"}, "cell_type": "markdown", "source": "## Create the meta data frame capturing the sensitive data"}, {"metadata": {"id": "37aa35f9cdfb466085acbb7eac3e56ec"}, "cell_type": "code", "source": "meta_df = df[protected_attributes].copy()\nmeta_fields = meta_df.columns.tolist()\nmeta_values = meta_df[meta_fields].values.tolist()", "execution_count": null, "outputs": []}, {"metadata": {"id": "ee69bf1da4a04865a6db14f13c46908d"}, "cell_type": "markdown", "source": "## Construct the scoring payload comprising the meta fields"}, {"metadata": {"id": "3f50af2cb0f04203998c93f6caade4fb"}, "cell_type": "code", "source": "def get_scoring_payload(no_of_records_to_score = 1):\n    meta_payload = {\n        \"fields\": meta_fields,\n        \"values\": meta_values[:no_of_records_to_score]\n    }\n\n    for col in cols_to_remove:\n        if col in df.columns:\n            del df[col] \n\n    fields = df.columns.tolist()\n    values = df[fields].values.tolist()\n\n    payload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values[:no_of_records_to_score],\"meta\": meta_payload}]}  \n    return payload_scoring", "execution_count": null, "outputs": []}, {"metadata": {"id": "8ec0f7a9328546748ae07218e4061a9d"}, "cell_type": "code", "source": "deployment_uid", "execution_count": null, "outputs": []}, {"metadata": {"id": "90f239341ee04e13acb5455607cd4f25"}, "cell_type": "markdown", "source": "## Method to perform scoring"}, {"metadata": {"id": "ad3991e04720434c9bcd42c7acf44b59"}, "cell_type": "code", "source": "def sample_scoring(no_of_records_to_score = 1):\n    records_list=[]\n    payload_scoring = get_scoring_payload(no_of_records_to_score)\n    scoring_response = wml_client.deployments.score(deployment_uid, payload_scoring)\n    print('Scoring result:', '\\n fields:', scoring_response['predictions'][0]['fields'], '\\n values: ', scoring_response['predictions'][0]['values'][0])\n    print(json.dumps(scoring_response, indent=None))\n    return payload_scoring, scoring_response", "execution_count": null, "outputs": []}, {"metadata": {"id": "fccba0eaa7794d6c9bb5990c435aa563"}, "cell_type": "code", "source": "from ibm_watson_openscale.supporting_classes.payload_record import PayloadRecord\ndef payload_logging(no_of_records_to_score = 1):\n    records_list=[]\n    payload_scoring = get_scoring_payload(no_of_records_to_score)\n    \n    \n    scoring_response = wml_client.deployments.score(deployment_uid, payload_scoring)\n    time.sleep(10)\n    pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n    print(\"Number of records in the payload logging table: {}\".format(pl_records_count))\n    if pl_records_count == 0:\n        print(\"Payload logging did not happen, performing explicit payload logging.\")\n    \n        #manual PL logging if automated logging does not work\n        score_input=payload_scoring['input_data'][0]\n        score_response=scoring_response['predictions'][0]\n        pl_record = PayloadRecord(request=score_input, response=score_response, response_time=int(460))\n        records_list.append(pl_record)\n        wos_client.data_sets.store_records(data_set_id = payload_data_set_id, request_body=records_list)\n        \n        \n        time.sleep(10)\n        pl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\n        print(\"Number of records in the payload logging table: {}\".format(pl_records_count))", "execution_count": null, "outputs": []}, {"metadata": {"id": "bd59a8e3d0bb431eb3be9252b1f3772c"}, "cell_type": "markdown", "source": "## Score the model and print the scoring response"}, {"metadata": {"id": "9b1853a125a340e99c622b9f71167c9c"}, "cell_type": "code", "source": "sample_scoring(no_of_records_to_score = 1)", "execution_count": null, "outputs": []}, {"metadata": {"id": "7056440604434d4f8b9429befee9bdce"}, "cell_type": "markdown", "source": "# Configure OpenScale \n\nThe notebook will now import the necessary libraries and set up a Python OpenScale client."}, {"metadata": {"id": "e577b95129004af19aeccbb4ae4a7862"}, "cell_type": "code", "source": "from ibm_watson_openscale import APIClient\nfrom ibm_watson_openscale.utils import *\nfrom ibm_watson_openscale.supporting_classes import *\nfrom ibm_watson_openscale.supporting_classes.enums import *\n\nimport json\nimport requests\nimport base64\nfrom requests.auth import HTTPBasicAuth\nimport time", "execution_count": null, "outputs": []}, {"metadata": {"id": "3043544c667e49d38cd70855d8ccb5ad"}, "cell_type": "markdown", "source": "## Get a instance of the OpenScale SDK client"}, {"metadata": {"id": "d3d5b4df0a9f4d598a6cbd595700ae53"}, "cell_type": "code", "source": "authenticator = CloudPakForDataAuthenticator(\n        url=WOS_CREDENTIALS['url'],\n        username=WOS_CREDENTIALS['username'],\n        password=WOS_CREDENTIALS['password'],\n        disable_ssl_verification=True\n    )\n\nwos_client = APIClient(service_url=WOS_CREDENTIALS['url'],service_instance_id=\"00000000-0000-0000-0000-1611161485480737\", authenticator=authenticator)\nwos_client.version", "execution_count": null, "outputs": []}, {"metadata": {"id": "ca368ed3793443da944e24b8b5e0d281"}, "cell_type": "code", "source": "wos_client.data_marts.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "6de26fb20d6a4ec58a2741db611f036d"}, "cell_type": "code", "source": "DB_CREDENTIALS = None", "execution_count": null, "outputs": []}, {"metadata": {"id": "2966db92a2994a958136635144f05572"}, "cell_type": "code", "source": "data_marts = wos_client.data_marts.list().result.data_marts\nif len(data_marts) == 0:\n    if DB_CREDENTIALS is not None:\n        if SCHEMA_NAME is None: \n            print(\"Please specify the SCHEMA_NAME and rerun the cell\")\n\n        print('Setting up external datamart')\n        added_data_mart_result = wos_client.data_marts.add(\n                background_mode=False,\n                name=\"WOS Data Mart\",\n                description=\"Data Mart created by WOS tutorial notebook\",\n                database_configuration=DatabaseConfigurationRequest(\n                  database_type=DatabaseType.DB2,\n                    credentials=PrimaryStorageCredentialsLong(\n                        hostname=DATABASE_CREDENTIALS['hostname'],\n                        username=DATABASE_CREDENTIALS['username'],\n                        password=DATABASE_CREDENTIALS['password'],\n                        db=DATABASE_CREDENTIALS['database'],\n                        port=DATABASE_CREDENTIALS['port'],\n                        ssl=DATABASE_CREDENTIALS['ssl'],\n                        sslmode=DATABASE_CREDENTIALS['sslmode'],\n                        certificate_base64=DATABASE_CREDENTIALS['certificate_base64']\n                    ),\n                    location=LocationSchemaName(\n                        schema_name= SCHEMA_NAME\n                    )\n                )\n             ).result\n    else:\n        print('Setting up internal datamart')\n        added_data_mart_result = wos_client.data_marts.add(\n                background_mode=False,\n                name=\"WOS Data Mart\",\n                description=\"Data Mart created by WOS tutorial notebook\", \n                internal_database = True).result\n        \n    data_mart_id = added_data_mart_result.metadata.id\n    \nelse:\n    data_mart_id=data_marts[0].metadata.id\n    print('Using existing datamart {}'.format(data_mart_id))", "execution_count": null, "outputs": []}, {"metadata": {"id": "5862cdb82a534375b7e290cee338774e"}, "cell_type": "code", "source": "data_mart_details = wos_client.data_marts.list().result.data_marts[0]\ndata_mart_details.to_dict()", "execution_count": null, "outputs": []}, {"metadata": {"id": "b61551bb00fa430b8595748c93a2dfb9"}, "cell_type": "code", "source": "wos_client.service_providers.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "b1ecf806072240cd89af4324cb844744"}, "cell_type": "markdown", "source": "## Remove existing service provider connected with used WML instance.\n\nMultiple service providers for the same engine instance are avaiable in Watson OpenScale. To avoid multiple service providers of used WML instance in the tutorial notebook the following code deletes existing service provder(s) and then adds new one."}, {"metadata": {"id": "34534f784ac845438b54c6947d227ccb"}, "cell_type": "code", "source": "SERVICE_PROVIDER_NAME = \"WML - Indirect Bias Testing\"\nSERVICE_PROVIDER_DESCRIPTION = \"Added by tutorial WOS notebook to showcase Indirect Bias functionality.\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "ebd41ed59d0a44b3837152bdd67e10fc"}, "cell_type": "code", "source": "service_providers = wos_client.service_providers.list().result.service_providers\nfor service_provider in service_providers:\n    service_instance_name = service_provider.entity.name\n    if service_instance_name == SERVICE_PROVIDER_NAME:\n        service_provider_id = service_provider.metadata.id\n        wos_client.service_providers.delete(service_provider_id)\n        print(\"Deleted existing service_provider for WML instance: {}\".format(service_provider_id))", "execution_count": null, "outputs": []}, {"metadata": {"id": "48cf5770290744c28274669a5af0103b"}, "cell_type": "markdown", "source": "## Add service provider\n\nWatson OpenScale needs to be bound to the Watson Machine Learning instance to capture payload data into and out of the model.\nNote: You can bind more than one engine instance if needed by calling wos_client.service_providers.add method. Next, you can refer to particular service provider using service_provider_id."}, {"metadata": {"id": "2d260d321cad475682aea0857c9b1054"}, "cell_type": "code", "source": "added_service_provider_result = wos_client.service_providers.add(\n        name=SERVICE_PROVIDER_NAME,\n        description=SERVICE_PROVIDER_DESCRIPTION,\n        service_type=ServiceTypes.WATSON_MACHINE_LEARNING,\n        deployment_space_id = WML_SPACE_ID,\n        operational_space_id = \"pre_production\",\n        credentials=WML_CREDENTIALS,\n        background_mode=False\n    ).result\nservice_provider_id = added_service_provider_result.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {"id": "eae202864f724fb9880ed31fadf75ac0"}, "cell_type": "code", "source": "print(wos_client.service_providers.get(service_provider_id).result)", "execution_count": null, "outputs": []}, {"metadata": {"id": "4ced8049e546405e81129ada3cbf73d1"}, "cell_type": "code", "source": "asset_deployment_details = wos_client.service_providers.list_assets(data_mart_id=data_mart_id, service_provider_id=service_provider_id, deployment_id=deployment_uid, deployment_space_id = WML_SPACE_ID).result['resources'][0]\nasset_deployment_details", "execution_count": null, "outputs": []}, {"metadata": {"id": "5ed2cbb8cfbd40518178d204be799ecb"}, "cell_type": "code", "source": "model_asset_details_from_deployment=wos_client.service_providers.get_deployment_asset(data_mart_id=data_mart_id,service_provider_id=service_provider_id,deployment_id=deployment_uid,deployment_space_id=WML_SPACE_ID)", "execution_count": null, "outputs": []}, {"metadata": {"id": "09f4993d2caa48269b9832c7e9b145d6"}, "cell_type": "markdown", "source": "## Subscriptions"}, {"metadata": {"id": "2e59906593c44bda97f79a9c35e728f6"}, "cell_type": "markdown", "source": "Remove existing model subscriptions\n\nThis code removes previous subscriptions to the model to refresh the monitors with the new model and new data."}, {"metadata": {"id": "6b97d781684b401095188860bd0a3c96"}, "cell_type": "code", "source": "wos_client.subscriptions.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "b0d085c8a875458b834992abd3577219"}, "cell_type": "markdown", "source": "## Remove the existing subscription"}, {"metadata": {"id": "f5ef621a07cb4e468fca6cc62d22595e"}, "cell_type": "code", "source": "subscriptions = wos_client.subscriptions.list().result.subscriptions\nfor subscription in subscriptions:\n    sub_model_id = subscription.entity.asset.asset_id\n    if sub_model_id == model_uid:\n        wos_client.subscriptions.delete(subscription.metadata.id)\n        print('Deleted existing subscription for model', sub_model_id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d117477d751b4d6380d2feceac14450e"}, "cell_type": "markdown", "source": "This code creates the model subscription in OpenScale using the Python client API. Note that we need to provide the model unique identifier, and some information about the model itself."}, {"metadata": {"id": "e3d6cdba01c140a090a4e3e27a12bdcf"}, "cell_type": "code", "source": "feature_columns = cat_features + num_features\nfeature_columns", "execution_count": null, "outputs": []}, {"metadata": {"id": "b7e200c3388d42c087f937ca745938c9"}, "cell_type": "code", "source": "subscription_details = wos_client.subscriptions.add(\n        data_mart_id=data_mart_id,\n        service_provider_id=service_provider_id,\n        asset=Asset(\n            asset_id=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"asset_id\"],\n            name=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"name\"],\n            url=model_asset_details_from_deployment[\"entity\"][\"asset\"][\"url\"],\n            asset_type=AssetTypes.MODEL,\n            input_data_type=InputDataType.STRUCTURED,\n            problem_type=ProblemType.BINARY_CLASSIFICATION\n        ),\n        deployment=AssetDeploymentRequest(\n            deployment_id=asset_deployment_details['metadata']['guid'],\n            name=asset_deployment_details['entity']['name'],\n            deployment_type= DeploymentTypes.ONLINE,\n            url=asset_deployment_details['entity']['scoring_endpoint']['url']\n        ),\n        asset_properties=AssetPropertiesRequest(\n            label_column=\"HIRED\",\n            probability_fields=[\"probability\"],\n            prediction_field=\"predictedLabel\",\n            feature_fields = feature_columns,\n            categorical_fields = cat_features,\n            training_data_reference=TrainingDataReference(type=\"cos\",\n                                                          location=COSTrainingDataReferenceLocation(bucket = BUCKET_NAME,\n                                                                                                    file_name = FILE_NAME),\n                                                          connection=COSTrainingDataReferenceConnection.from_dict({\n                                                                        \"resource_instance_id\": COS_RESOURCE_CRN,\n                                                                        \"url\": COS_ENDPOINT,\n                                                                        \"api_key\": COS_API_KEY_ID,\n                                                                        \"iam_url\": IAM_URL})),\n            training_data_schema=SparkStruct.from_dict(model_asset_details_from_deployment[\"entity\"][\"asset_properties\"][\"training_data_schema\"])\n        )\n    ).result\nsubscription_id = subscription_details.metadata.id\nprint('subscription_id: ' + subscription_id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "9dee41fc8ae2407483f7d08f7ecd9663"}, "cell_type": "code", "source": "import time\n\ntime.sleep(5)\npayload_data_set_id = None\npayload_data_set_id = wos_client.data_sets.list(type=DataSetTypes.PAYLOAD_LOGGING, \n                                                target_target_id=subscription_id, \n                                                target_target_type=TargetTypes.SUBSCRIPTION).result.data_sets[0].metadata.id\nif payload_data_set_id is None:\n    print(\"Payload data set not found. Please check subscription status.\")\nelse:\n    print(\"Payload data set id:\", payload_data_set_id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "2e2d47b3928c4f6c87fe3379168f2301"}, "cell_type": "code", "source": "wos_client.data_sets.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "5ba479997dca4e2899796ba7c050fed5"}, "cell_type": "code", "source": "wos_client.subscriptions.get(subscription_id).result.to_dict()", "execution_count": null, "outputs": []}, {"metadata": {"id": "2d75c57ab8914d70acdc3ed523675b1b"}, "cell_type": "markdown", "source": "# Score the model so we can configure monitors\n\nNow that the WML service has been bound and the subscription has been created, we need to send a request to the model before we configure OpenScale. This allows OpenScale to create a payload log in the datamart with the correct schema, so it can capture data coming into and out of the model."}, {"metadata": {"id": "14ef8cf50f5748589f34ab3c15fe5d2f"}, "cell_type": "code", "source": "payload_logging(no_of_records_to_score = 10)", "execution_count": null, "outputs": []}, {"metadata": {"id": "933b5220cd9d4c5681f54308ce266ade"}, "cell_type": "code", "source": "time.sleep(5)\npl_records_count = wos_client.data_sets.get_records_count(payload_data_set_id)\nprint(\"Number of records in the payload logging table: {}\".format(pl_records_count))\nif pl_records_count == 0:\n    raise Exception(\"Payload logging did not happen!\")", "execution_count": null, "outputs": []}, {"metadata": {"id": "09c318b2e5c54b839eb6f3ac8b247ca4"}, "cell_type": "markdown", "source": "## Fairness configuration"}, {"metadata": {"id": "3d73b84061cc494a8ea5a19e897b981f"}, "cell_type": "markdown", "source": "The code below configures fairness monitoring for our model. It turns on monitoring for two features, sex and age. In each case, we must specify:\n    \nWhich model feature to monitor One or more majority groups, which are values of that feature that we expect to receive a higher percentage of favorable outcomes One or more minority groups, which are values of that feature that we expect to receive a higher percentage of unfavorable outcomes The threshold at which we would like OpenScale to display an alert if the fairness measurement falls below (in this case, 80%) Additionally, we must specify which outcomes from the model are favourable outcomes, and which are unfavourable. We must also provide the number of records OpenScale will use to calculate the fairness score. In this case, OpenScale's fairness monitor will run hourly, but will not calculate a new fairness rating until at least 100 records have been added. Finally, to calculate fairness, OpenScale must perform some calculations on the training data, so we provide the dataframe containing the data."}, {"metadata": {"id": "f59d4d82ca7d4aa7978e7a777416c281"}, "cell_type": "markdown", "source": "### Create Fairness Monitor Instance"}, {"metadata": {"id": "5845b8fc9ad742d280a45182cfbad2e1"}, "cell_type": "code", "source": "target = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n)\nparameters = {\n    \"features\": [\n        {\n            \"feature\": \"Gender\",\n            \"majority\": [\"Male\"],\n            \"minority\": [\"Female\"]\n        },\n        {\n            \"feature\": \"Ethnicity\",\n            \"majority\": [\"non-minority\"],\n            \"minority\": [\"minority\"]\n        }\n    ],\n    \"favourable_class\": [\"YES\"],\n    \"unfavourable_class\": [\"NO\"],\n    \"min_records\": 100\n}\n\nthresholds = [\n    {\n        \"metric_id\": \"fairness_value\",\n        \"specific_values\": [\n           {\n                \"applies_to\": [\n                    {\n                        \"type\": \"tag\",\n                        \"value\": \"Gender\",\n                        \"key\": \"feature\"\n                    }\n                ],\n                \"value\": 80\n            },\n            {\n                \"applies_to\": [\n                    {\n                        \"type\": \"tag\",\n                        \"value\": \"Ethnicity\",\n                        \"key\": \"feature\"\n                    }\n                ],\n                \"value\": 80\n            }\n        ],\n        \"type\": \"lower_limit\",\n        \"value\": 80\n    }\n]\n\nfairness_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.FAIRNESS.ID,\n    target=target,\n    parameters=parameters,\n    thresholds=thresholds\n).result\n\nfairness_monitor_instance_id = fairness_monitor_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {"id": "ae999234ea39453e82c74169ac7d74a8"}, "cell_type": "markdown", "source": "### Get Fairness Monitor Instance"}, {"metadata": {"id": "812cdeea78af4ca8ba38ea5f07efd990"}, "cell_type": "code", "source": "wos_client.monitor_instances.show()", "execution_count": null, "outputs": []}, {"metadata": {"id": "890aa1236420457fb2550326d30cffa9"}, "cell_type": "markdown", "source": "### Drift Configuration"}, {"metadata": {"id": "30fbd73e9be14151b70edc98770bb43b"}, "cell_type": "code", "source": "monitor_instances = wos_client.monitor_instances.list().result.monitor_instances\nfor monitor_instance in monitor_instances:\n    monitor_def_id=monitor_instance.entity.monitor_definition_id\n    if monitor_def_id == \"drift\" and monitor_instance.entity.target.target_id == subscription_id:\n        wos_client.monitor_instances.delete(monitor_instance.metadata.id)\n        print('Deleted existing drift monitor instance with id: ', monitor_instance.metadata.id)\n\n\ntarget = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n\n)\n\nparameters = {\n    \"min_samples\": 100,\n    \"drift_threshold\": 0.05,\n    \"train_drift_model\": True,\n    \"enable_model_drift\": True,\n    \"enable_data_drift\": True\n}\n\ndrift_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.DRIFT.ID,\n    target=target,\n    parameters=parameters\n).result\n\ndrift_monitor_instance_id = drift_monitor_details.metadata.id\ndrift_monitor_instance_id", "execution_count": null, "outputs": []}, {"metadata": {"id": "1a4ec2f042b74bc886ffcddf5a9afbdc"}, "cell_type": "markdown", "source": "## Enable quality monitoring\nThe code below turns on the quality (accuracy) monitor and sets an alert threshold of 80%. OpenScale will show an alert on the dashboard if the model accuracy measurement (area under the curve, in the case of a binary classifier) falls below this threshold.\n\nThe second paramater supplied, min_records, specifies the minimum number of feedback records OpenScale needs before it calculates a new measurement. The quality monitor runs hourly, but the accuracy reading in the dashboard will not change until an additional 50 feedback records have been added, via the user interface, the Python client, or the supplied feedback endpoint."}, {"metadata": {"id": "af75679ceb7d44a08a4c72c5608864cd"}, "cell_type": "code", "source": "target = Target(\n        target_type=TargetTypes.SUBSCRIPTION,\n        target_id=subscription_id\n)\nparameters = {\n    \"min_feedback_data_size\": 50\n}\nthresholds = [\n    {\n        \"metric_id\": \"area_under_roc\",\n        \"type\": \"lower_limit\",\n        \"value\": 0.8\n    }\n]\nquality_monitor_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.QUALITY.ID,\n    target=target,\n    parameters=parameters,\n    thresholds=thresholds \n).result\nquality_monitor_instance_id = quality_monitor_details.metadata.id\nquality_monitor_instance_id", "execution_count": null, "outputs": []}, {"metadata": {"id": "3e59ec9f42d64baa816f5a428f7c1f4c"}, "cell_type": "markdown", "source": "## Configure Explainability\nFinally, we provide OpenScale with the training data to enable and configure the explainability features."}, {"metadata": {"id": "76e60e7f701b4a1698f2397d25b68c09"}, "cell_type": "code", "source": "target = Target(\n    target_type=TargetTypes.SUBSCRIPTION,\n    target_id=subscription_id\n)\nparameters = {\n    \"enabled\": True\n}\nexplainability_details = wos_client.monitor_instances.create(\n    data_mart_id=data_mart_id,\n    background_mode=False,\n    monitor_definition_id=wos_client.monitor_definitions.MONITORS.EXPLAINABILITY.ID,\n    target=target,\n    parameters=parameters\n).result\n\nexplainability_monitor_id = explainability_details.metadata.id", "execution_count": null, "outputs": []}, {"metadata": {"id": "33c838b779344eed80d844e9534f3d49"}, "cell_type": "markdown", "source": "# STOP HERE! Upload and evaluate test data\n\nAt this point, you can navigate to the **Instances** section of your cluster and open OpenScale. From the OpenScale Insights Dashboard, and select the **Hiring Deployment - Indirect Debias** model. From the **Actions** menu, choose **Evaluate now**. Select **from CSV** from the **Import** dropdown, and upload the [payload_100.csv](https://raw.githubusercontent.com/ericmartens/indirect-bias/main/data/payload_100.csv) file. Then click **Upload and evaluate**. The monitors will take a few minutes to run, but when the screen refreshes, you will see information on the test results for fairness, quality and drift, along with two generated explanations.\n\nWhen the tests have finished running, you may continue with the steps below to test the auto-debiased endpoint."}, {"metadata": {"id": "52084fdbd03e4b1d93ed2de03100dc93"}, "cell_type": "markdown", "source": "## Test the model and auto-debiased endpoing results using a record that will produce a biased result\n\nNow that we've run the model test, OpenScale's auto-debiased endpoint is active. We can see how this works by sending records to the deployed model that will produce a biased result, then sending the same records to the debiased endpoint and seeing that we get a different prediction.\n\nFirst, we'll get the scoring endpoint for the deployed model."}, {"metadata": {"id": "b94417673e044fa099e69c5e967e1168"}, "cell_type": "code", "source": "subscriptions = wos_client.subscriptions.list().result.subscriptions\n\nspace_id = None\ndeployment_id = None\ndatamart_id = None\nsubscription_id = None\nfor subscription in subscriptions:\n    if subscription.entity.deployment.name == deployment_name:\n        deployment_id = subscription.entity.deployment.deployment_id\n        space_id = subscription.entity.asset.url.split('?space_id=')[1].split('&version')[0]\n        datamart_id = subscription.entity.data_mart_id\n        subscription_id = subscription.metadata.id\n        print(\"Deployment ID:\", deployment_id)\n        print(\"Space ID:\", space_id)\n        print(\"Datamart ID:\", datamart_id)\n        print(\"Subscription ID:\", subscription_id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "59c47f0e8dfa43f7bda374d71d21ea95"}, "cell_type": "code", "source": "wml_client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {"id": "d81b37592acd47c69bef1d43cd4f561e"}, "cell_type": "code", "source": "fields = ['Age', 'BusinessTravel', 'Department', 'DistanceFromHome', 'Education',\n          'EducationField', 'RelevantEducationLevel', 'JobLevel', 'JobRole', 'MaritalStatus', 'NumCompaniesWorked', 'OverTime', 'InterviewScore',\n          'ResumeScore', 'RequestedBenefits', 'TotalWorkingYears', 'PreferredSkills', 'YearsAtCurrentCompany', 'RelevantExperience', 'JobType', 'SalaryExpectation']", "execution_count": null, "outputs": []}, {"metadata": {"id": "d23484f0897644208c63342165960ebd"}, "cell_type": "markdown", "source": "Next, we'll get 50 records to send to our production model and our debiased endpoint."}, {"metadata": {"id": "75aeca43ffc34a08879c6d1a10d94bb6"}, "cell_type": "code", "source": "no_of_records_to_score = 50\nscoring_payload = get_scoring_payload(no_of_records_to_score)", "execution_count": null, "outputs": []}, {"metadata": {"id": "12253049ef884e9181bf70fea4a290f1"}, "cell_type": "markdown", "source": "Send the records to the production model for scoring."}, {"metadata": {"id": "8593da5606a34e9e8ac4459168b479e4"}, "cell_type": "code", "source": "scoring_response = wml_client.deployments.score(deployment_id, scoring_payload)", "execution_count": null, "outputs": []}, {"metadata": {"id": "7acb92c4a0324a998b1a68475c5645ba"}, "cell_type": "markdown", "source": "Next, we'll get an authentication token so we can use the auto-debiased endpoint."}, {"metadata": {"id": "e41f97bd3a114fa08c4fdc4aae8f3001"}, "cell_type": "code", "source": "import json\nimport requests\nimport base64\nfrom requests.auth import HTTPBasicAuth\nimport time\n\ntoken_url = WOS_CREDENTIALS['url'] + '/v1/preauth/validateAuth'\nheaders = {}\nheaders[\"Accept\"] = \"application/json\"\nauth = HTTPBasicAuth(WOS_CREDENTIALS['username'], WOS_CREDENTIALS['password'])\nresponse = requests.get(token_url, headers=headers, auth=auth, verify=False)\njson_data = response.json()\naccess_token = json_data['accessToken']", "execution_count": null, "outputs": []}, {"metadata": {"id": "fd4e6dd338844121b9d7d93025a67609"}, "cell_type": "code", "source": "DEBIASING_PREDICTIONS_URL = WOS_CREDENTIALS['url'] + \"/openscale/{0}/v2/subscriptions/{1}/predictions\".format(data_mart_id,subscription_id)\nprint(DEBIASING_PREDICTIONS_URL)\n\nheaders = {}\nheaders[\"Content-Type\"] = \"application/json\"\nheaders[\"Accept\"] = \"application/json\"\nheaders[\"Authorization\"] = \"Bearer {}\".format(access_token)\n\ndebiased_scoring_payload = scoring_payload['input_data'][0]\nresponse = requests.post(DEBIASING_PREDICTIONS_URL, data=json.dumps(debiased_scoring_payload), headers=headers, verify=False)", "execution_count": null, "outputs": []}, {"metadata": {"id": "c72d25d4bd09450c8d565d3cd5f3e91f"}, "cell_type": "markdown", "source": "Find the and print the records where the model prediction differs from the debiased prediction."}, {"metadata": {"id": "27be23a2af1740e18fee68645393726b"}, "cell_type": "code", "source": "predictedLabel_index = response.json()['fields'].index('predictedLabel')\ndebiased_prediction_index = response.json()['fields'].index('debiased_prediction')\n\nfor j in range(no_of_records_to_score):\n    scored_record = response.json()['values'][j]\n    predictedLabel = scored_record[predictedLabel_index]\n    debiased_prediction = scored_record[debiased_prediction_index]\n    if predictedLabel != debiased_prediction:\n        print('==========')\n        print(scored_record)\n        print('predictedLabel:' + str(predictedLabel) + ', debiased_prediction=' + str(debiased_prediction))\n        print('==========')", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}